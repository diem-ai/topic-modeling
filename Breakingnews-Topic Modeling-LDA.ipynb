{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Every document we read can be thought of as consisting of many topics all stacked upon one another. Today, we’re going can unpack these topics using of NLP techniques: \n",
    "- Latent Dirichlet Allocation (LDA) and Topic Modeling\n",
    "- Data is collected on https://www.reuters.com/breakingviews by a scrapping script\n",
    "- The goal is to break text documents down into topics by word. \n",
    "- What is laten feature ? Mathematically, we want to find “topics” that are collections of words that appear in similar documents. \n",
    "  More generally, it is a collection of features in a dataset.\n",
    "- There are several libraries for LDA such as scikit-learn and gensim. I choose gensim for this project. \n",
    "\n",
    "#### Project tasks:\n",
    "- Cleaning the dataset & Lemmatization\n",
    "- Creat a dictionay from processed data\n",
    "- Create Corpus and LDA Model with bag of words\n",
    "- Create Coprpus and LDA with TF-IDF\n",
    "- Caculate the Perplexity and Topic Cohenrence between two models\n",
    "- Visualize topics with the help of pyLDAvis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\btdiem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import string\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import unidecode\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('../data/breakingnews.csv', error_bad_lines=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Joseph Stiglitz does what he does very well. T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>For all the successes of Japanese Prime Minist...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           headline\n",
       "0           0  Joseph Stiglitz does what he does very well. T...\n",
       "1           1  For all the successes of Japanese Prime Minist..."
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text = data[['headline']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Preprocessing Data & Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<input>:57: DeprecationWarning: invalid escape sequence \\S\n",
      "<input>:62: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:57: DeprecationWarning: invalid escape sequence \\S\n",
      "<input>:62: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:57: DeprecationWarning: invalid escape sequence \\S\n",
      "<input>:62: DeprecationWarning: invalid escape sequence \\s\n",
      "<ipython-input-89-422b4386cee7>:57: DeprecationWarning: invalid escape sequence \\S\n",
      "  email_re =  re.compile('\\S*@\\S*\\s?')\n",
      "<ipython-input-89-422b4386cee7>:62: DeprecationWarning: invalid escape sequence \\s\n",
      "  newline_re = re.compile('\\s+')\n"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Convert the part-of-speech naming scheme\n",
    "       from the nltk default to that which is\n",
    "       recognized by the WordNet lemmatizer\"\"\"\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "      \n",
    "# remove alpha numerical words and make lowercase\n",
    "alphanum_re = re.compile(r\"\"\"\\w*\\d\\w*\"\"\")\n",
    "alphanum_lambda = lambda x: alphanum_re.sub('', x)\n",
    "\n",
    "re_alpha = re.compile('[^A-Za-z]', re.UNICODE)\n",
    "alphaonly = lambda x : re_alpha.sub(' ', x)\n",
    "\n",
    "# remove punctuation\n",
    "punc_re = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "punc_lambda = lambda x: punc_re.sub(' ', x)\n",
    "\n",
    "single_quote1 = re.compile(\"’\")\n",
    "nosinglequote1 = lambda x : re.sub(single_quote1 , '', x)\n",
    "\n",
    "single_quote2 = re.compile('‘*')\n",
    "nosinglequote2 = lambda x : re.sub(single_quote2 , '', x)\n",
    "\n",
    "\n",
    "double_quote = re.compile('[\"]*')\n",
    "nodoublequote = lambda x : re.sub(double_quote , '', x)\n",
    "\n",
    "# remove stop words\n",
    "sw = stopwords.words('english')\n",
    "sw_lambda = lambda x: list(filter(lambda y: y not in sw, x))\n",
    "\n",
    "pos_lambda = lambda x: [(y[0], get_wordnet_pos(y[1])) for y in x]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lem_lambda = lambda x: [lemmatizer.lemmatize(*y) for y in x]\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_raw_data(data):\n",
    "    \"\"\"\n",
    "    data: Pandas series\n",
    "    \"\"\"\n",
    "     # remove email\n",
    "    email_re =  re.compile('\\S*@\\S*\\s?')\n",
    "    noemail = lambda x : email_re.sub(' ', x)\n",
    "    data = data.map(noemail)\n",
    " \n",
    "    # remove new line character:\n",
    "    newline_re = re.compile('\\s+')\n",
    "    nonewline = lambda x : newline_re.sub(' ', x)\n",
    "    data = data.map(nonewline)\n",
    "    # Remove distracting single quotes\n",
    "    sg_quote_re = re.compile(\"\\'\")\n",
    "    no_sg_quote = lambda x : sg_quote_re.sub(' ', x)\n",
    "    data = data.map(no_sg_quote)\n",
    "    \n",
    "    data = data.map(simple_preprocess)\n",
    "    \n",
    "    # remove stop words\n",
    "#    data = data.map(word_tokenize)\n",
    "    sw = stopwords.words('english')\n",
    "    sw_lambda = lambda x: list(filter(lambda y: y not in sw, x))\n",
    "    # tokenize words before removing stopwords\n",
    "    data = data.map(sw_lambda)\n",
    "\n",
    "    # part of speech tagging--must convert to format used by lemmatizer\n",
    "    data = data.map(nltk.pos_tag)\n",
    "    data = data.map(pos_lambda)\n",
    "    # lemmatization\n",
    "    data = data.map(lem_lambda)\n",
    "    \n",
    "    return data\n",
    " \n",
    "def get_score(lda_model, doc2vec):\n",
    "    \"\"\"\n",
    "    lda_model: LDA model \n",
    "    \n",
    "    \"\"\"\n",
    "    for index, score in sorted(lda_model[doc2vec], key=lambda tup: -1*tup[1]):\n",
    "        print(\"\\nScore: {}\\nTopic: {} \\nWord: {}\".format(score, index, lda_model.print_topic(index, 10)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = preprocess_raw_data(documents['headline'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Dictionary and Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create a corpus from a list of texts\n",
    "dictionary = corpora.Dictionary(processed_docs)\n",
    "# filter out the less common words\n",
    "# Keep tokens which are contained in at least 15 documents\n",
    "# Keep tokens which are contained in no more than 50% documents\n",
    "# Keep only the first 10000 most frequent tokens\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=10000)\n",
    "# Term Document Frequency, it is a list of (word_id, word_frequency) in the processed_docs.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "# View the first document in corpus\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('may', 1), ('provide', 1)]]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see what words from given ids in dictionary and their frequency\n",
    "[[(dictionary[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA Topic Modeling (Bag of words)\n",
    "- Building LDA using Bag of Words with 5 topics\n",
    "- LDA model is built with 5 different topics where each topic is a combination of keywords and each keyword contributes a certain weightage to the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(corpus\n",
    "                                       , num_topics=5\n",
    "                                       , id2word=dictionary\n",
    "                                       , iterations=50\n",
    "                                       , passes=2\n",
    "                                       , workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### View the topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic: 0\n",
      "Words: 0.647*\"year\" + 0.086*\"may\" + 0.052*\"billion\" + 0.051*\"news\" + 0.051*\"provide\" + 0.051*\"big\" + 0.021*\"world\" + 0.021*\"com\" + 0.020*\"day\"\n",
      "\n",
      "Topic: 1\n",
      "Words: 0.245*\"world\" + 0.196*\"news\" + 0.147*\"day\" + 0.147*\"com\" + 0.099*\"provide\" + 0.057*\"billion\" + 0.057*\"big\" + 0.050*\"may\" + 0.002*\"year\"\n",
      "\n",
      "Topic: 2\n",
      "Words: 0.404*\"may\" + 0.401*\"provide\" + 0.043*\"year\" + 0.042*\"big\" + 0.042*\"news\" + 0.018*\"com\" + 0.017*\"world\" + 0.017*\"billion\" + 0.017*\"day\"\n",
      "\n",
      "Topic: 3\n",
      "Words: 0.317*\"billion\" + 0.315*\"may\" + 0.170*\"com\" + 0.169*\"year\" + 0.006*\"big\" + 0.006*\"news\" + 0.006*\"provide\" + 0.006*\"world\" + 0.006*\"day\"\n",
      "\n",
      "Topic: 4\n",
      "Words: 0.349*\"big\" + 0.183*\"news\" + 0.182*\"year\" + 0.182*\"billion\" + 0.048*\"may\" + 0.036*\"provide\" + 0.007*\"com\" + 0.007*\"world\" + 0.007*\"day\"\n"
     ]
    }
   ],
   "source": [
    "#pprint(lda_model.print_topics())\n",
    "for idx, topic in lda_model.print_topics():\n",
    "    print('\\nTopic: {}\\nWords: {}'.format(idx+1, topic))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Model Perplexity and Coherence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -2.5590968742763436\n",
      "\n",
      "Coherence Score:  0.4224021185538137\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "# a measure of how good the model is. the lower, the better.\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus)) \n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model\n",
    "                                     , corpus=corpus\n",
    "                                     , texts = list(processed_docs)\n",
    "                                     , dictionary=dictionary \n",
    "                                     ,coherence='c_v')\n",
    "\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the topics-keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el1141617507932228083807518267\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el1141617507932228083807518267_data = {\"mdsDat\": {\"x\": [0.20834803169366767, -0.11279614802445194, -0.05260753465310792, 0.08426130250020442, -0.12720565151631227], \"y\": [-0.0698815752754808, 0.0918319027689113, -0.1372041093627633, 0.1530307001557225, -0.03777691828638986], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [55.292545318603516, 14.730565071105957, 14.699700355529785, 9.598559379577637, 5.678625583648682]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"Freq\": [19.0, 20.0, 25.0, 18.0, 22.0, 29.0, 22.0, 28.0, 17.0, 27.58951759338379, 16.623342514038086, 22.104799270629883, 16.61842155456543, 11.128610610961914, 6.467888355255127, 6.484293460845947, 5.583499431610107, 0.19642052054405212, 9.537374496459961, 9.480559349060059, 5.083280086517334, 5.094484329223633, 0.16801689565181732, 0.17343662679195404, 0.171517014503479, 0.16994239389896393, 0.17174212634563446, 10.470394134521484, 5.460446357727051, 5.455941677093506, 5.47593879699707, 1.4312478303909302, 1.0870767831802368, 0.200731560587883, 0.203227698802948, 0.2023826539516449, 7.846518039703369, 7.909015655517578, 0.821456253528595, 0.8399620056152344, 0.8152070045471191, 0.3291652202606201, 0.3437540829181671, 0.33792731165885925, 0.3380565643310547, 7.494662761688232, 0.9913212656974792, 0.5910040140151978, 0.5910652279853821, 0.6072903871536255, 0.5961406826972961, 0.23740236461162567, 0.2376609891653061, 0.2378489375114441], \"Term\": [\"year\", \"provide\", \"may\", \"big\", \"billion\", \"news\", \"com\", \"world\", \"day\", \"world\", \"day\", \"news\", \"com\", \"provide\", \"big\", \"billion\", \"may\", \"year\", \"billion\", \"may\", \"year\", \"com\", \"day\", \"big\", \"provide\", \"world\", \"news\", \"big\", \"year\", \"billion\", \"news\", \"may\", \"provide\", \"day\", \"com\", \"world\", \"provide\", \"may\", \"big\", \"year\", \"news\", \"day\", \"com\", \"billion\", \"world\", \"year\", \"may\", \"big\", \"provide\", \"billion\", \"news\", \"day\", \"com\", \"world\"], \"Total\": [19.0, 20.0, 25.0, 18.0, 22.0, 29.0, 22.0, 28.0, 17.0, 28.537748336791992, 17.558656692504883, 29.16382598876953, 22.497549057006836, 20.824787139892578, 18.524179458618164, 22.422826766967773, 25.39564323425293, 19.074771881103516, 22.422826766967773, 25.39564323425293, 19.074771881103516, 22.497549057006836, 17.558656692504883, 18.524179458618164, 20.824787139892578, 28.537748336791992, 29.16382598876953, 18.524179458618164, 19.074771881103516, 22.422826766967773, 29.16382598876953, 25.39564323425293, 20.824787139892578, 17.558656692504883, 22.497549057006836, 28.537748336791992, 20.824787139892578, 25.39564323425293, 18.524179458618164, 19.074771881103516, 29.16382598876953, 17.558656692504883, 22.497549057006836, 22.422826766967773, 28.537748336791992, 19.074771881103516, 25.39564323425293, 18.524179458618164, 20.824787139892578, 22.422826766967773, 29.16382598876953, 17.558656692504883, 22.497549057006836, 28.537748336791992], \"loglift\": [9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.5587000250816345, 0.5378000140190125, 0.31540000438690186, 0.2896000146865845, -0.0340999998152256, -0.45969998836517334, -0.6481999754905701, -0.9222000241279602, -3.983299970626831, 1.0604000091552734, 0.9298999905586243, 0.5928000211715698, 0.4300000071525574, -2.7339999675750732, -2.7558000087738037, -2.884000062942505, -3.2083001136779785, -3.219399929046631, 1.3467999696731567, 0.6664999723434448, 0.5040000081062317, 0.24480000138282776, -0.9587000012397766, -1.0353000164031982, -2.553999900817871, -2.7894999980926514, -3.0315001010894775, 1.3674999475479126, 1.1770000457763672, -0.7721999883651733, -0.77920001745224, -1.2337000370025635, -1.6332000494003296, -1.8377000093460083, -1.8514000177383423, -2.0922000408172607, 1.9342999458312988, -0.3747999966144562, -0.5764999985694885, -0.6934999823570251, -0.7404000163078308, -1.0217000246047974, -1.4350999593734741, -1.6819000244140625, -1.9189000129699707], \"logprob\": [9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -1.4082000255584717, -1.9148000478744507, -1.6297999620437622, -1.9150999784469604, -2.3160998821258545, -2.8587000370025635, -2.8561999797821045, -3.0058000087738037, -6.353099822998047, -1.1476999521255493, -1.1535999774932861, -1.776900053024292, -1.7747000455856323, -5.186600208282471, -5.154799938201904, -5.165900230407715, -5.17519998550415, -5.164599895477295, -1.0521999597549438, -1.7031999826431274, -1.7041000127792358, -1.7003999948501587, -3.0422000885009766, -3.3173000812530518, -5.0065999031066895, -4.994200229644775, -4.9984002113342285, -0.9144999980926514, -0.9065999984741211, -3.1712000370025635, -3.1489999294281006, -3.1789000034332275, -4.0858001708984375, -4.042399883270264, -4.059500217437744, -4.059100151062012, -0.43549999594688416, -2.458400011062622, -2.975600004196167, -2.9755001068115234, -2.9484000205993652, -2.966900110244751, -3.887700080871582, -3.8866000175476074, -3.8857998847961426]}, \"token.table\": {\"Topic\": [1, 3, 4, 5, 1, 2, 3, 5, 1, 2, 1, 1, 2, 3, 4, 5, 1, 3, 4, 5, 1, 3, 4, 5, 1, 2, 3, 4, 5], \"Freq\": [0.3239009976387024, 0.5398349761962891, 0.053983498364686966, 0.053983498364686966, 0.2675844728946686, 0.44597411155700684, 0.22298705577850342, 0.044597409665584564, 0.7556378841400146, 0.2222464382648468, 0.9681833982467651, 0.23626099526882172, 0.354391485452652, 0.03937683254480362, 0.31501466035842896, 0.03937683254480362, 0.7543591856956482, 0.17144526541233063, 0.034289054572582245, 0.034289054572582245, 0.5282166600227356, 0.04801969975233078, 0.38415759801864624, 0.04801969975233078, 0.981156587600708, 0.2621263265609741, 0.2621263265609741, 0.052425265312194824, 0.36697685718536377], \"Term\": [\"big\", \"big\", \"big\", \"big\", \"billion\", \"billion\", \"billion\", \"billion\", \"com\", \"com\", \"day\", \"may\", \"may\", \"may\", \"may\", \"may\", \"news\", \"news\", \"news\", \"news\", \"provide\", \"provide\", \"provide\", \"provide\", \"world\", \"year\", \"year\", \"year\", \"year\"]}, \"R\": 9, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 4, 5, 3, 1]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el1141617507932228083807518267\", ldavis_el1141617507932228083807518267_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el1141617507932228083807518267\", ldavis_el1141617507932228083807518267_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el1141617507932228083807518267\", ldavis_el1141617507932228083807518267_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "1      0.208348 -0.069882       1        1  55.292545\n",
       "3     -0.112796  0.091832       2        1  14.730565\n",
       "4     -0.052608 -0.137204       3        1  14.699700\n",
       "2      0.084261  0.153031       4        1   9.598559\n",
       "0     -0.127206 -0.037777       5        1   5.678626, topic_info=  Category       Freq     Term      Total  loglift  logprob\n",
       "7  Default  19.000000     year  19.000000   9.0000   9.0000\n",
       "1  Default  20.000000  provide  20.000000   8.0000   8.0000\n",
       "0  Default  25.000000      may  25.000000   7.0000   7.0000\n",
       "8  Default  18.000000      big  18.000000   6.0000   6.0000\n",
       "5  Default  22.000000  billion  22.000000   5.0000   5.0000\n",
       "3  Default  29.000000     news  29.000000   4.0000   4.0000\n",
       "6  Default  22.000000      com  22.000000   3.0000   3.0000\n",
       "4  Default  28.000000    world  28.000000   2.0000   2.0000\n",
       "2  Default  17.000000      day  17.000000   1.0000   1.0000\n",
       "4   Topic1  27.589518    world  28.537748   0.5587  -1.4082\n",
       "2   Topic1  16.623343      day  17.558657   0.5378  -1.9148\n",
       "3   Topic1  22.104799     news  29.163826   0.3154  -1.6298\n",
       "6   Topic1  16.618422      com  22.497549   0.2896  -1.9151\n",
       "1   Topic1  11.128611  provide  20.824787  -0.0341  -2.3161\n",
       "8   Topic1   6.467888      big  18.524179  -0.4597  -2.8587\n",
       "5   Topic1   6.484293  billion  22.422827  -0.6482  -2.8562\n",
       "0   Topic1   5.583499      may  25.395643  -0.9222  -3.0058\n",
       "7   Topic1   0.196421     year  19.074772  -3.9833  -6.3531\n",
       "5   Topic2   9.537374  billion  22.422827   1.0604  -1.1477\n",
       "0   Topic2   9.480559      may  25.395643   0.9299  -1.1536\n",
       "7   Topic2   5.083280     year  19.074772   0.5928  -1.7769\n",
       "6   Topic2   5.094484      com  22.497549   0.4300  -1.7747\n",
       "2   Topic2   0.168017      day  17.558657  -2.7340  -5.1866\n",
       "8   Topic2   0.173437      big  18.524179  -2.7558  -5.1548\n",
       "1   Topic2   0.171517  provide  20.824787  -2.8840  -5.1659\n",
       "4   Topic2   0.169942    world  28.537748  -3.2083  -5.1752\n",
       "3   Topic2   0.171742     news  29.163826  -3.2194  -5.1646\n",
       "8   Topic3  10.470394      big  18.524179   1.3468  -1.0522\n",
       "7   Topic3   5.460446     year  19.074772   0.6665  -1.7032\n",
       "5   Topic3   5.455942  billion  22.422827   0.5040  -1.7041\n",
       "3   Topic3   5.475939     news  29.163826   0.2448  -1.7004\n",
       "0   Topic3   1.431248      may  25.395643  -0.9587  -3.0422\n",
       "1   Topic3   1.087077  provide  20.824787  -1.0353  -3.3173\n",
       "2   Topic3   0.200732      day  17.558657  -2.5540  -5.0066\n",
       "6   Topic3   0.203228      com  22.497549  -2.7895  -4.9942\n",
       "4   Topic3   0.202383    world  28.537748  -3.0315  -4.9984\n",
       "1   Topic4   7.846518  provide  20.824787   1.3675  -0.9145\n",
       "0   Topic4   7.909016      may  25.395643   1.1770  -0.9066\n",
       "8   Topic4   0.821456      big  18.524179  -0.7722  -3.1712\n",
       "7   Topic4   0.839962     year  19.074772  -0.7792  -3.1490\n",
       "3   Topic4   0.815207     news  29.163826  -1.2337  -3.1789\n",
       "2   Topic4   0.329165      day  17.558657  -1.6332  -4.0858\n",
       "6   Topic4   0.343754      com  22.497549  -1.8377  -4.0424\n",
       "5   Topic4   0.337927  billion  22.422827  -1.8514  -4.0595\n",
       "4   Topic4   0.338057    world  28.537748  -2.0922  -4.0591\n",
       "7   Topic5   7.494663     year  19.074772   1.9343  -0.4355\n",
       "0   Topic5   0.991321      may  25.395643  -0.3748  -2.4584\n",
       "8   Topic5   0.591004      big  18.524179  -0.5765  -2.9756\n",
       "1   Topic5   0.591065  provide  20.824787  -0.6935  -2.9755\n",
       "5   Topic5   0.607290  billion  22.422827  -0.7404  -2.9484\n",
       "3   Topic5   0.596141     news  29.163826  -1.0217  -2.9669\n",
       "2   Topic5   0.237402      day  17.558657  -1.4351  -3.8877\n",
       "6   Topic5   0.237661      com  22.497549  -1.6819  -3.8866\n",
       "4   Topic5   0.237849    world  28.537748  -1.9189  -3.8858, token_table=      Topic      Freq     Term\n",
       "term                          \n",
       "8         1  0.323901      big\n",
       "8         3  0.539835      big\n",
       "8         4  0.053983      big\n",
       "8         5  0.053983      big\n",
       "5         1  0.267584  billion\n",
       "5         2  0.445974  billion\n",
       "5         3  0.222987  billion\n",
       "5         5  0.044597  billion\n",
       "6         1  0.755638      com\n",
       "6         2  0.222246      com\n",
       "2         1  0.968183      day\n",
       "0         1  0.236261      may\n",
       "0         2  0.354391      may\n",
       "0         3  0.039377      may\n",
       "0         4  0.315015      may\n",
       "0         5  0.039377      may\n",
       "3         1  0.754359     news\n",
       "3         3  0.171445     news\n",
       "3         4  0.034289     news\n",
       "3         5  0.034289     news\n",
       "1         1  0.528217  provide\n",
       "1         3  0.048020  provide\n",
       "1         4  0.384158  provide\n",
       "1         5  0.048020  provide\n",
       "4         1  0.981157    world\n",
       "7         2  0.262126     year\n",
       "7         3  0.262126     year\n",
       "7         4  0.052425     year\n",
       "7         5  0.366977     year, R=9, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[2, 4, 5, 3, 1])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA model with TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('may', 0.6309418522375851), ('provide', 0.7758301225751714)]]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[(dictionary[id], freq) for id, freq in cp] for cp in corpus_tfidf[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_tfidf = gensim.models.LdaMulticore(corpus_tfidf\n",
    "                                       , num_topics=5\n",
    "                                       , id2word=dictionary\n",
    "                                       , iterations=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View the topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic: 1\n",
      "Words: 0.484*\"may\" + 0.200*\"billion\" + 0.170*\"provide\" + 0.067*\"big\" + 0.017*\"year\" + 0.015*\"world\" + 0.015*\"com\" + 0.015*\"news\" + 0.015*\"day\"\n",
      "\n",
      "Topic: 2\n",
      "Words: 0.521*\"year\" + 0.200*\"news\" + 0.200*\"big\" + 0.015*\"may\" + 0.014*\"provide\" + 0.013*\"billion\" + 0.012*\"world\" + 0.012*\"com\" + 0.012*\"day\"\n",
      "\n",
      "Topic: 3\n",
      "Words: 0.317*\"may\" + 0.247*\"billion\" + 0.177*\"provide\" + 0.177*\"big\" + 0.018*\"year\" + 0.016*\"world\" + 0.016*\"com\" + 0.016*\"news\" + 0.016*\"day\"\n",
      "\n",
      "Topic: 4\n",
      "Words: 0.193*\"billion\" + 0.190*\"com\" + 0.146*\"year\" + 0.142*\"may\" + 0.093*\"world\" + 0.089*\"big\" + 0.069*\"day\" + 0.047*\"news\" + 0.031*\"provide\"\n",
      "\n",
      "Topic: 5\n",
      "Words: 0.243*\"world\" + 0.214*\"news\" + 0.158*\"day\" + 0.130*\"com\" + 0.092*\"provide\" + 0.062*\"may\" + 0.046*\"big\" + 0.046*\"billion\" + 0.008*\"year\"\n"
     ]
    }
   ],
   "source": [
    "#pprint(lda_model.print_topics())\n",
    "for idx, topic in lda_tfidf.print_topics():\n",
    "    print('\\nTopic: {}\\nWords: {}'.format(idx+1, topic))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Perplexity & Coherence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -3.186376803651969\n",
      "\n",
      "Coherence Score:  0.4224021185538137\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nTopics = 15\\n\\nPerplexity:  -9.27488119057329\\n\\nCoherence Score:  0.24405747657214635\\n\\nTopics = 10\\n\\nPerplexity:  -9.00745369196948\\n\\nCoherence Score:  0.2058747841509882\\n\\nTopics = 20\\n\\nPerplexity:  -9.656468835436096\\n\\nCoherence Score:  0.3316541111751868\\n\\n'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "# a measure of how good the model is. the lower, the better.\n",
    "print('\\nPerplexity: ', lda_tfidf.log_perplexity(corpus_tfidf)) \n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_tfidf = CoherenceModel(model=lda_tfidf\n",
    "                                     , corpus=corpus_tfidf\n",
    "                                     , texts = list(processed_docs)\n",
    "                                     , dictionary=dictionary \n",
    "                                     ,coherence='c_v')\n",
    "\n",
    "coherence_tfidf = coherence_model_tfidf.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el1141617506147658802884136453\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el1141617506147658802884136453_data = {\"mdsDat\": {\"x\": [-0.026095579297661917, 0.17897419870807274, -0.260958464598303, -0.01775663624980114, 0.12583648143769363], \"y\": [0.1749602439710171, -0.04259933929644022, -0.07954144242773374, 0.013374776602621782, -0.06619423884946497], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [33.870201110839844, 21.097637176513672, 17.67460823059082, 14.493518829345703, 12.864039421081543]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"Freq\": [11.0, 17.0, 11.0, 7.0, 9.0, 11.0, 9.0, 9.0, 6.0, 7.877998352050781, 5.117868900299072, 6.936695575714111, 4.227954387664795, 2.98150372505188, 1.4970762729644775, 1.4837768077850342, 2.0133230686187744, 0.2711591422557831, 9.77145767211914, 3.4297754764556885, 4.037700653076172, 1.3609797954559326, 0.30470338463783264, 0.3111252188682556, 0.31274983286857605, 0.34706515073776245, 0.31087952852249146, 8.815497398376465, 3.3842227458953857, 3.384805202484131, 0.20572109520435333, 0.2082921415567398, 0.2300705760717392, 0.21031494438648224, 0.22233931720256805, 0.24998313188552856, 2.634326219558716, 2.673433303833008, 2.027501344680786, 0.9625992774963379, 1.2835121154785156, 1.2309366464614868, 1.9708231687545776, 0.6579962968826294, 0.42641934752464294, 3.045400381088257, 2.181804895401001, 2.1754512786865234, 3.90142560005188, 0.1923973709344864, 0.19775637984275818, 0.19811812043190002, 0.2234744131565094, 0.19261710345745087], \"Term\": [\"year\", \"may\", \"billion\", \"com\", \"world\", \"news\", \"provide\", \"big\", \"day\", \"world\", \"day\", \"news\", \"com\", \"provide\", \"big\", \"billion\", \"may\", \"year\", \"may\", \"provide\", \"billion\", \"big\", \"day\", \"com\", \"world\", \"year\", \"news\", \"year\", \"big\", \"news\", \"day\", \"com\", \"provide\", \"world\", \"billion\", \"may\", \"com\", \"billion\", \"year\", \"day\", \"world\", \"big\", \"may\", \"news\", \"provide\", \"billion\", \"provide\", \"big\", \"may\", \"day\", \"com\", \"world\", \"year\", \"news\"], \"Total\": [11.0, 17.0, 11.0, 7.0, 9.0, 11.0, 9.0, 9.0, 6.0, 9.88269329071045, 6.783289909362793, 11.482994079589844, 7.579453945159912, 9.249573707580566, 9.648666381835938, 11.462650299072266, 17.907012939453125, 11.684697151184082, 17.907012939453125, 9.249573707580566, 11.462650299072266, 9.648666381835938, 6.783289909362793, 7.579453945159912, 9.88269329071045, 11.684697151184082, 11.482994079589844, 11.684697151184082, 9.648666381835938, 11.482994079589844, 6.783289909362793, 7.579453945159912, 9.249573707580566, 9.88269329071045, 11.462650299072266, 17.907012939453125, 7.579453945159912, 11.462650299072266, 11.684697151184082, 6.783289909362793, 9.88269329071045, 9.648666381835938, 17.907012939453125, 11.482994079589844, 9.249573707580566, 11.462650299072266, 9.249573707580566, 9.648666381835938, 17.907012939453125, 6.783289909362793, 7.579453945159912, 9.88269329071045, 11.684697151184082, 11.482994079589844], \"loglift\": [9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.85589998960495, 0.8008999824523926, 0.5785999894142151, 0.49889999628067017, -0.0494999997317791, -0.7807000279426575, -0.961899995803833, -1.1028000116348267, -2.6807000637054443, 0.9502999782562256, 0.5638999938964844, 0.5126000046730042, -0.4025999903678894, -1.5469000339508057, -1.6369999647140503, -1.8970999717712402, -1.9605000019073486, -2.0532000064849854, 1.451300024986267, 0.6852999925613403, 0.5115000009536743, -1.7626999616622925, -1.861199975013733, -1.9608999490737915, -2.1168999671936035, -2.2095999717712402, -2.5385000705718994, 0.8747000098228455, 0.4756999909877777, 0.18000000715255737, -0.0210999995470047, -0.10970000177621841, -0.12759999930858612, -0.275299996137619, -0.9279999732971191, -1.145400047302246, 0.7253000140190125, 0.6062999963760376, 0.5612000226974487, 0.5268999934196472, -1.5118999481201172, -1.5953999757766724, -1.8588999509811401, -1.906000018119812, -2.0371999740600586], \"logprob\": [9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -1.4142999649047852, -1.8456000089645386, -1.541599988937378, -2.0367000102996826, -2.385999917984009, -3.074899911880493, -3.0838000774383545, -2.778599977493286, -4.783400058746338, -0.7254999876022339, -1.7725000381469727, -1.6093000173568726, -2.6967999935150146, -4.193399906158447, -4.172599792480469, -4.167399883270264, -4.063300132751465, -4.173399925231934, -0.6514999866485596, -1.6088999509811401, -1.6087000370025635, -4.409200191497803, -4.3968000411987305, -4.297299861907959, -4.3871002197265625, -4.331500053405762, -4.214300155639648, -1.6608999967575073, -1.6461999416351318, -1.922700047492981, -2.6677000522613525, -2.380000114440918, -2.421799898147583, -1.9510999917984009, -3.048099994659424, -3.4818999767303467, -1.3967000246047974, -1.7301000356674194, -1.7330000400543213, -1.148900032043457, -4.1585001945495605, -4.13100004196167, -4.129199981689453, -4.008699893951416, -4.157299995422363]}, \"token.table\": {\"Topic\": [1, 2, 3, 4, 5, 1, 2, 4, 5, 1, 4, 1, 4, 1, 2, 4, 5, 1, 3, 4, 1, 2, 5, 1, 4, 3, 4], \"Freq\": [0.10364126414060593, 0.10364126414060593, 0.3109237849712372, 0.10364126414060593, 0.20728252828121185, 0.08723986148834229, 0.34895944595336914, 0.26171958446502686, 0.26171958446502686, 0.5277425050735474, 0.3958068788051605, 0.7371054291725159, 0.14742109179496765, 0.11168808490037918, 0.5584404468536377, 0.11168808490037918, 0.22337616980075836, 0.6095970869064331, 0.2612558901309967, 0.0870852991938591, 0.32433927059173584, 0.32433927059173584, 0.21622617542743683, 0.8094959259033203, 0.10118699073791504, 0.7702382206916809, 0.17116403579711914], \"Term\": [\"big\", \"big\", \"big\", \"big\", \"big\", \"billion\", \"billion\", \"billion\", \"billion\", \"com\", \"com\", \"day\", \"day\", \"may\", \"may\", \"may\", \"may\", \"news\", \"news\", \"news\", \"provide\", \"provide\", \"provide\", \"world\", \"world\", \"year\", \"year\"]}, \"R\": 9, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [5, 1, 2, 4, 3]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el1141617506147658802884136453\", ldavis_el1141617506147658802884136453_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el1141617506147658802884136453\", ldavis_el1141617506147658802884136453_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el1141617506147658802884136453\", ldavis_el1141617506147658802884136453_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "4     -0.026096  0.174960       1        1  33.870201\n",
       "0      0.178974 -0.042599       2        1  21.097637\n",
       "1     -0.260958 -0.079541       3        1  17.674608\n",
       "3     -0.017757  0.013375       4        1  14.493519\n",
       "2      0.125836 -0.066194       5        1  12.864039, topic_info=  Category       Freq     Term      Total  loglift  logprob\n",
       "7  Default  11.000000     year  11.000000   9.0000   9.0000\n",
       "0  Default  17.000000      may  17.000000   8.0000   8.0000\n",
       "5  Default  11.000000  billion  11.000000   7.0000   7.0000\n",
       "6  Default   7.000000      com   7.000000   6.0000   6.0000\n",
       "4  Default   9.000000    world   9.000000   5.0000   5.0000\n",
       "3  Default  11.000000     news  11.000000   4.0000   4.0000\n",
       "1  Default   9.000000  provide   9.000000   3.0000   3.0000\n",
       "8  Default   9.000000      big   9.000000   2.0000   2.0000\n",
       "2  Default   6.000000      day   6.000000   1.0000   1.0000\n",
       "4   Topic1   7.877998    world   9.882693   0.8559  -1.4143\n",
       "2   Topic1   5.117869      day   6.783290   0.8009  -1.8456\n",
       "3   Topic1   6.936696     news  11.482994   0.5786  -1.5416\n",
       "6   Topic1   4.227954      com   7.579454   0.4989  -2.0367\n",
       "1   Topic1   2.981504  provide   9.249574  -0.0495  -2.3860\n",
       "8   Topic1   1.497076      big   9.648666  -0.7807  -3.0749\n",
       "5   Topic1   1.483777  billion  11.462650  -0.9619  -3.0838\n",
       "0   Topic1   2.013323      may  17.907013  -1.1028  -2.7786\n",
       "7   Topic1   0.271159     year  11.684697  -2.6807  -4.7834\n",
       "0   Topic2   9.771458      may  17.907013   0.9503  -0.7255\n",
       "1   Topic2   3.429775  provide   9.249574   0.5639  -1.7725\n",
       "5   Topic2   4.037701  billion  11.462650   0.5126  -1.6093\n",
       "8   Topic2   1.360980      big   9.648666  -0.4026  -2.6968\n",
       "2   Topic2   0.304703      day   6.783290  -1.5469  -4.1934\n",
       "6   Topic2   0.311125      com   7.579454  -1.6370  -4.1726\n",
       "4   Topic2   0.312750    world   9.882693  -1.8971  -4.1674\n",
       "7   Topic2   0.347065     year  11.684697  -1.9605  -4.0633\n",
       "3   Topic2   0.310880     news  11.482994  -2.0532  -4.1734\n",
       "7   Topic3   8.815497     year  11.684697   1.4513  -0.6515\n",
       "8   Topic3   3.384223      big   9.648666   0.6853  -1.6089\n",
       "3   Topic3   3.384805     news  11.482994   0.5115  -1.6087\n",
       "2   Topic3   0.205721      day   6.783290  -1.7627  -4.4092\n",
       "6   Topic3   0.208292      com   7.579454  -1.8612  -4.3968\n",
       "1   Topic3   0.230071  provide   9.249574  -1.9609  -4.2973\n",
       "4   Topic3   0.210315    world   9.882693  -2.1169  -4.3871\n",
       "5   Topic3   0.222339  billion  11.462650  -2.2096  -4.3315\n",
       "0   Topic3   0.249983      may  17.907013  -2.5385  -4.2143\n",
       "6   Topic4   2.634326      com   7.579454   0.8747  -1.6609\n",
       "5   Topic4   2.673433  billion  11.462650   0.4757  -1.6462\n",
       "7   Topic4   2.027501     year  11.684697   0.1800  -1.9227\n",
       "2   Topic4   0.962599      day   6.783290  -0.0211  -2.6677\n",
       "4   Topic4   1.283512    world   9.882693  -0.1097  -2.3800\n",
       "8   Topic4   1.230937      big   9.648666  -0.1276  -2.4218\n",
       "0   Topic4   1.970823      may  17.907013  -0.2753  -1.9511\n",
       "3   Topic4   0.657996     news  11.482994  -0.9280  -3.0481\n",
       "1   Topic4   0.426419  provide   9.249574  -1.1454  -3.4819\n",
       "5   Topic5   3.045400  billion  11.462650   0.7253  -1.3967\n",
       "1   Topic5   2.181805  provide   9.249574   0.6063  -1.7301\n",
       "8   Topic5   2.175451      big   9.648666   0.5612  -1.7330\n",
       "0   Topic5   3.901426      may  17.907013   0.5269  -1.1489\n",
       "2   Topic5   0.192397      day   6.783290  -1.5119  -4.1585\n",
       "6   Topic5   0.197756      com   7.579454  -1.5954  -4.1310\n",
       "4   Topic5   0.198118    world   9.882693  -1.8589  -4.1292\n",
       "7   Topic5   0.223474     year  11.684697  -1.9060  -4.0087\n",
       "3   Topic5   0.192617     news  11.482994  -2.0372  -4.1573, token_table=      Topic      Freq     Term\n",
       "term                          \n",
       "8         1  0.103641      big\n",
       "8         2  0.103641      big\n",
       "8         3  0.310924      big\n",
       "8         4  0.103641      big\n",
       "8         5  0.207283      big\n",
       "5         1  0.087240  billion\n",
       "5         2  0.348959  billion\n",
       "5         4  0.261720  billion\n",
       "5         5  0.261720  billion\n",
       "6         1  0.527743      com\n",
       "6         4  0.395807      com\n",
       "2         1  0.737105      day\n",
       "2         4  0.147421      day\n",
       "0         1  0.111688      may\n",
       "0         2  0.558440      may\n",
       "0         4  0.111688      may\n",
       "0         5  0.223376      may\n",
       "3         1  0.609597     news\n",
       "3         3  0.261256     news\n",
       "3         4  0.087085     news\n",
       "1         1  0.324339  provide\n",
       "1         2  0.324339  provide\n",
       "1         5  0.216226  provide\n",
       "4         1  0.809496    world\n",
       "4         4  0.101187    world\n",
       "7         3  0.770238     year\n",
       "7         4  0.171164     year, R=9, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[5, 1, 2, 4, 3])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_tfidf, corpus_tfidf, dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic: 1\n",
      "Words: 0.484*\"may\" + 0.200*\"billion\" + 0.170*\"provide\" + 0.067*\"big\" + 0.017*\"year\" + 0.015*\"world\" + 0.015*\"com\" + 0.015*\"news\" + 0.015*\"day\"\n",
      "\n",
      "Topic: 2\n",
      "Words: 0.521*\"year\" + 0.200*\"news\" + 0.200*\"big\" + 0.015*\"may\" + 0.014*\"provide\" + 0.013*\"billion\" + 0.012*\"world\" + 0.012*\"com\" + 0.012*\"day\"\n",
      "\n",
      "Topic: 3\n",
      "Words: 0.317*\"may\" + 0.247*\"billion\" + 0.177*\"provide\" + 0.177*\"big\" + 0.018*\"year\" + 0.016*\"world\" + 0.016*\"com\" + 0.016*\"news\" + 0.016*\"day\"\n",
      "\n",
      "Topic: 4\n",
      "Words: 0.193*\"billion\" + 0.190*\"com\" + 0.146*\"year\" + 0.142*\"may\" + 0.093*\"world\" + 0.089*\"big\" + 0.069*\"day\" + 0.047*\"news\" + 0.031*\"provide\"\n",
      "\n",
      "Topic: 5\n",
      "Words: 0.243*\"world\" + 0.214*\"news\" + 0.158*\"day\" + 0.130*\"com\" + 0.092*\"provide\" + 0.062*\"may\" + 0.046*\"big\" + 0.046*\"billion\" + 0.008*\"year\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_tfidf.print_topics(-1):\n",
    "    print('\\nTopic: {}\\nWords: {}'.format(idx+1, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1), (2, 1), (4, 2), (6, 2), (8, 1)]\n",
      "provide 1\n",
      "day 1\n",
      "world 2\n",
      "com 2\n",
      "big 1\n",
      "\n",
      "Score: 0.89858078956604\n",
      "Topic: 1 \n",
      "Word: 0.245*\"world\" + 0.196*\"news\" + 0.147*\"day\" + 0.147*\"com\" + 0.099*\"provide\" + 0.057*\"billion\" + 0.057*\"big\" + 0.050*\"may\" + 0.002*\"year\"\n",
      "\n",
      "Score: 0.025722915306687355\n",
      "Topic: 4 \n",
      "Word: 0.349*\"big\" + 0.183*\"news\" + 0.182*\"year\" + 0.182*\"billion\" + 0.048*\"may\" + 0.036*\"provide\" + 0.007*\"com\" + 0.007*\"world\" + 0.007*\"day\"\n",
      "\n",
      "Score: 0.025433992967009544\n",
      "Topic: 2 \n",
      "Word: 0.404*\"may\" + 0.401*\"provide\" + 0.043*\"year\" + 0.042*\"big\" + 0.042*\"news\" + 0.018*\"com\" + 0.017*\"world\" + 0.017*\"billion\" + 0.017*\"day\"\n",
      "\n",
      "Score: 0.025217458605766296\n",
      "Topic: 3 \n",
      "Word: 0.317*\"billion\" + 0.315*\"may\" + 0.170*\"com\" + 0.169*\"year\" + 0.006*\"big\" + 0.006*\"news\" + 0.006*\"provide\" + 0.006*\"world\" + 0.006*\"day\"\n",
      "\n",
      "Score: 0.025044839829206467\n",
      "Topic: 0 \n",
      "Word: 0.647*\"year\" + 0.086*\"may\" + 0.052*\"billion\" + 0.051*\"news\" + 0.051*\"provide\" + 0.051*\"big\" + 0.021*\"world\" + 0.021*\"com\" + 0.020*\"day\"\n"
     ]
    }
   ],
   "source": [
    "#Make a test\n",
    "print(corpus[10])\n",
    "[print(dictionary[id], freq) for id, freq in corpus[10]]\n",
    "\n",
    "get_score(lda_model, corpus[10])\n",
    "#for index, score in sorted(lda_model[common_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "#    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 0.30151134457776363), (2, 0.30151134457776363), (4, 0.6030226891555273), (6, 0.6030226891555273), (8, 0.30151134457776363)]\n",
      "provide 0.30151134457776363\n",
      "day 0.30151134457776363\n",
      "world 0.6030226891555273\n",
      "com 0.6030226891555273\n",
      "big 0.30151134457776363\n",
      "\n",
      "Score: 0.7362167835235596\n",
      "Topic: 4 \n",
      "Word: 0.243*\"world\" + 0.214*\"news\" + 0.158*\"day\" + 0.130*\"com\" + 0.092*\"provide\" + 0.062*\"may\" + 0.046*\"big\" + 0.046*\"billion\" + 0.008*\"year\"\n",
      "\n",
      "Score: 0.06635069847106934\n",
      "Topic: 3 \n",
      "Word: 0.193*\"billion\" + 0.190*\"com\" + 0.146*\"year\" + 0.142*\"may\" + 0.093*\"world\" + 0.089*\"big\" + 0.069*\"day\" + 0.047*\"news\" + 0.031*\"provide\"\n",
      "\n",
      "Score: 0.06631165742874146\n",
      "Topic: 2 \n",
      "Word: 0.317*\"may\" + 0.247*\"billion\" + 0.177*\"provide\" + 0.177*\"big\" + 0.018*\"year\" + 0.016*\"world\" + 0.016*\"com\" + 0.016*\"news\" + 0.016*\"day\"\n",
      "\n",
      "Score: 0.06595547497272491\n",
      "Topic: 1 \n",
      "Word: 0.521*\"year\" + 0.200*\"news\" + 0.200*\"big\" + 0.015*\"may\" + 0.014*\"provide\" + 0.013*\"billion\" + 0.012*\"world\" + 0.012*\"com\" + 0.012*\"day\"\n",
      "\n",
      "Score: 0.06516535580158234\n",
      "Topic: 0 \n",
      "Word: 0.484*\"may\" + 0.200*\"billion\" + 0.170*\"provide\" + 0.067*\"big\" + 0.017*\"year\" + 0.015*\"world\" + 0.015*\"com\" + 0.015*\"news\" + 0.015*\"day\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(corpus_tfidf[10])\n",
    "[print(dictionary[id], freq) for id, freq in corpus_tfidf[10]]\n",
    "\n",
    "get_score(lda_tfidf, corpus_tfidf[10])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
