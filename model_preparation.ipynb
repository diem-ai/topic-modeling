{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_preparation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diem-ai/topic-modeling/blob/master/model_preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f28PG4o7x4SB",
        "colab_type": "text"
      },
      "source": [
        "#### This note book will clean data and create bag of word and tf-idf sparse matrix and save them with help of Pickle. They will be used again when we build LDA, LSA and NMF model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sC9PMZHNeVTx",
        "colab_type": "text"
      },
      "source": [
        "#### Google Colab Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Olt7Xq5byPKU",
        "colab_type": "code",
        "outputId": "2be8142c-8b71-45be-9064-d6dbdaf52361",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "# This will prompt for authorization.\n",
        "# authorization code: 4/OwErfUj6QceGXhIGx_RWv0MKclb9rilw8UsJnZqFbSez-QS8zQ399JU\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-X20vwCyxDa",
        "colab_type": "code",
        "outputId": "fd006cb3-5628-435e-932a-d8fe329b76ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        }
      },
      "source": [
        "!pip install PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyDrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.6.7)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.11.3)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.12.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.5)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiERuDAhef71",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import accessory_functions.py from Colab\n",
        "#https://drive.google.com/open?id=1S7URZIBq4zMh5QWv0qXPHv4ixhgHWN_y\n",
        "\n",
        "my_module = drive.CreateFile({'id':'1S7URZIBq4zMh5QWv0qXPHv4ixhgHWN_y'})\n",
        "my_module.GetContentFile('accessory_functions.py')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqAGU-zax4SE",
        "colab_type": "text"
      },
      "source": [
        "####  Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXVkDzd8x4SG",
        "colab_type": "code",
        "outputId": "be910cff-7eab-4bf1-ebce-ba585bb72676",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        }
      },
      "source": [
        "import numpy as np\n",
        "import string\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "import gensim.corpora as corpora\n",
        "\n",
        "\n",
        "import pickle\n",
        "from wordcloud import WordCloud\n",
        "# Plotting tools\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "# Make all my plots 538 Style\n",
        "plt.style.use('fivethirtyeight')\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "from accessory_functions import preprocess_series_text\n",
        "from accessory_functions import write_pickle_file\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-japK8GCUDwR",
        "colab_type": "text"
      },
      "source": [
        "<p> Data Path </p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ys_ThbPcULFn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datapath = '/content/drive/My Drive/data/'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fafPRjT9x4SW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(datapath + 'breakingnews.csv')\n",
        "\n",
        "data.head(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7soRO8Byx4Sv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_text = data[['headline_text']]\n",
        "data_text['index'] = data_text.index\n",
        "documents = data_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSF22i0Ix4S3",
        "colab_type": "text"
      },
      "source": [
        "#####  cleaning data and save it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUWPftAIx4TE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processed_docs = preprocess_series_text(documents['headline_text'])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRO2xCacvwpG",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-IG7oaQvv7v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(15, 15))\n",
        "wordcloud = WordCloud(\n",
        "        background_color='white'\n",
        "#        max_words=200,\n",
        "        ,max_font_size=20\n",
        "        , scale=2) # chosen at random by flipping a coin; it was heads)\n",
        "\n",
        "wordcloud.generate(str(processed_docs))\n",
        "\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHe0MTHHyNnd",
        "colab_type": "text"
      },
      "source": [
        "<p> From the visualization, we can guess the topics should be about Donald Trump, Uber, Bank, Tesla, minister. We will check it out in another notebook. Now we save the processed_docs oject for latter use</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hbo1KRhgyxo5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processed_docs = processed_docs.values.tolist()\n",
        "\n",
        "write_pickle_file(processed_docs, datapath + 'processed_docs.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9R9QuxNx4TN",
        "colab_type": "text"
      },
      "source": [
        "<p> Create gensim dictionary  from processed_docs</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ii1EdPEx4TQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# filter out the less common words\n",
        "# Keep tokens which are contained in at least 15 documents\n",
        "# Keep tokens which are contained in no more than 50% documents\n",
        "# Keep only the first 10000 most frequent tokens\n",
        "#dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=10000)\n",
        "dictionary = corpora.Dictionary(processed_docs)\n",
        "\n",
        "print(\"1st word in dictionary: {}\".format(dictionary[0]))\n",
        "\n",
        "print(\"1OOth word in dictionary: {}\".format(dictionary[100]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_D_jUBoUk_uw",
        "colab_type": "text"
      },
      "source": [
        "<p>Convert processed_docs into bad of word (bow) using dictionary </p>\n",
        "<p> It is a list of word id and theirs frequencies </p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGEaKO4pkiJK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bow = [dictionary.doc2bow(doc) for doc in processed_docs]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yS8UxuZCmslK",
        "colab_type": "text"
      },
      "source": [
        "<p>View the first document in bow and using dictionary to see which words and their frequency</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1YoTWCWotDQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc = bow[1]\n",
        "\n",
        "[(dictionary[id], freq) for id, freq in doc]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zf7V4Dfesvge",
        "colab_type": "text"
      },
      "source": [
        "<p> Save bag-of_word </p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vj6MM8XUskmt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "write_pickle_file(bow, datapath+'bow.pkl')\n",
        "\n",
        "\"\"\"\n",
        "for id, freq in doc:\n",
        "  print(\"{} : {}\".format(dictionary[id], freq))\n",
        "\"\"\"\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0VAQXA3tPAQ",
        "colab_type": "text"
      },
      "source": [
        "<p>We build Tf-Idf, a simple transformation which takes documents represented as bag-of-words counts and applies a weighting which discounts common terms or equivalently, promotes rare terms </p>\n",
        "<p> Then, save tfidf for using later</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqJe11DBtjWk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf_model = gensim.models.TfidfModel(bow)\n",
        "tfidf = tfidf_model[bow]\n",
        "\n",
        "write_pickle_file(tfidf, datapath + \"tfidf.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUS4cxaOt4MR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc = tfidf[1]\n",
        "[(dictionary[id], freq) for id, freq in doc]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCCYBWC01Obf",
        "colab_type": "text"
      },
      "source": [
        "<p>Finally, save gensim dictionary object</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgzTZi6m1W8h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "write_pickle_file(dictionary, datapath + 'dictionary.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}